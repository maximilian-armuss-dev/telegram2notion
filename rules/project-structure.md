# ðŸ¤– KI-Entwicklungsrichtlinien (Projekt-Spezifisch) ðŸ¤–

---

## 1. ProjektÃ¼bersicht & Architektur

Bevor du arbeitest, mache dich mit dem Projekt vertraut.

### 1.1. High-Level-Ziel

Dieses Projekt ist ein asynchroner Python-Agent, der Nachrichten (Text und Sprache) aus einem Telegram-Chat extrahiert, sie mittels eines LLM und RAG in strukturierte Notion-DatenbankeintrÃ¤ge umwandelt und diese Ã¼ber die Notion-API synchronisiert.

### 1.2. Verzeichnisstruktur & Verantwortlichkeiten

Die gesamte Anwendungslogik befindet sich im Verzeichnis `/app`.

```
/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                     # ðŸš€ Einstiegspunkt (Logging + Runtime-Start)
â”‚   â”œâ”€â”€ bootstrap.py                # Polling-Catch-up & Webhook-Server konfigurieren
â”‚   â”œâ”€â”€ config.py                   # âœ… Zentraler Zugriff auf .env-Konfiguration
â”‚   â”œâ”€â”€ logging_config.py           # Globale Logging-Konfiguration
â”‚   â”œâ”€â”€ cache_model.py              # HuggingFace-Embedding beim Build cachen
â”‚   â”œâ”€â”€ models.py                   # Pydantic-Modelle fÃ¼r Notion-Strukturen
â”‚   â”œâ”€â”€ state_manager.py            # Persistiert verarbeitete Telegram-Updates
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ telegram_service.py     # Telegram Bot API
â”‚   â”‚   â”œâ”€â”€ gladia_service.py       # Gladia Speech-to-Text
â”‚   â”‚   â”œâ”€â”€ llm_service.py          # LLM-Interaktion (LangChain)
â”‚   â”‚   â”œâ”€â”€ notion_service.py       # Notion API
â”‚   â”‚   â””â”€â”€ vector_service.py       # RAG-Index (FAISS + Embeddings)
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ workflow_processor.py   # ðŸ’¡ HERZSTÃœCK: orchestriert den Workflow
â”‚   â””â”€â”€ webhook_api.py              # FastAPI fÃ¼r Webhook & Health
â”œâ”€â”€ prompts/                        # LLM-Prompts (`gemini_prompt.md`, `thought_structuring_prompt.md`)
â”œâ”€â”€ scripts/                        # Hilfsskripte (`entrypoint.sh`, `inspect_security_logs.py`)
â”œâ”€â”€ .env                            # Speichert alle Secrets und Konfigurationen und darf nie gelesen werden
â”œâ”€â”€ .env.example                    # Beispiel fÃ¼r eine .env Datei, darf gelesen werden
â””â”€â”€ ...
```

### 1.3. Kern-Workflow (End-to-End)

1.  **Start:** `main.py` ruft den `WorkflowProcessor` in `workflow_processor.py` auf.
2.  **Setup:** Der Prozessor initialisiert alle Services und baut den RAG-Vektorindex auf.
3.  **Fetch:** Neue Nachrichten werden von Telegram geholt.
4.  **Content Extraction:** Text wird direkt verwendet; Sprachnachrichten werden transkribiert.
5.  **RAG Context:** Relevante Dokumente werden Ã¼ber den `VectorService` aus Notion geholt.
6.  **LLM Processing:** Der `LLMService` generiert aus den Gedanken und dem Kontext eine Liste von validierten Notion-Aktionen.
7.  **Execution:** Der `NotionService` fÃ¼hrt diese Aktionen aus (create, update, archive).
8.  **State Update:** Die IDs der verarbeiteten Nachrichten werden gespeichert.

---

## 2. Architekturprinzipien

-   **Separation of Concerns:** Halte die Verantwortlichkeiten strikt getrennt. `workflow_processor.py` orchestriert nur. Die gesamte Logik fÃ¼r externe APIs (Telegram, Notion, Gladia) gehÃ¶rt ausschlieÃŸlich in die entsprechenden Service-Klassen in `/app/services`.
-   **Keine Hartcodierung:** Alle Konfigurationswerte, API-Keys, Dateipfade, Modellnamen oder "magische" Strings mÃ¼ssen aus der `.env`-Datei Ã¼ber das `settings`-Objekt aus `app/config.py` geladen werden.
-   **Logging Ã¼ber `print()`:** Die `print()`-Funktion ist verboten. Verwende fÃ¼r jegliche Ausgabe das `logging`-Modul (`logging.info`, `logging.error`).
-   **SDKs bevorzugen:** Nutze immer die offiziellen SDKs (`python-telegram-bot`, `notion-client`), anstatt direkte HTTP-Anfragen mit `httpx` oder `requests` zu implementieren.
